\begin{itemize}
    \item \textbf{Support Vector Machine (SVM)}: O SVM é um classificador robusto que busca encontrar o hiperplano que melhor separa as classes. Sua capacidade de lidar com conjuntos de dados de alta dimensionalidade e sua robustez em relação a outliers foram consideradas vantagens importantes para nossa análise.
    \[
        \min_{\mathbf{w}, b} \frac{1}{2} \|\mathbf{w}\|^2 \quad \text{sujeito a} \quad y_i (\mathbf{w} \cdot \mathbf{x}_i + b) \geq 1
    \]
    onde $\mathbf{w}$ é o vetor de pesos, $b$ é o viés, $\mathbf{x}_i$ são os vetores de atributos e $y_i$ são as etiquetas de classe.
    
    \item \textbf{Random Forest}: Um conjunto de árvores de decisão que melhora a precisão do modelo por meio do método de bagging.
    \[
        f(\mathbf{x}) = \sum_{m=1}^{M} \alpha_m I(\mathbf{x} \in R_m)
    \]
    onde $M$ é o número de folhas, $\alpha_m$ é a previsão associada à região $R_m$ e $I(\cdot)$ é a função indicadora.
    
    \item \textbf{Decision Tree}: Um classificador baseado em regras que segmenta os dados de forma iterativa.
    \[
        \text{Gini}(t) = 1 - \sum_{i=1}^{C} p(i|t)^2
    \]
    onde $p(i|t)$ é a proporção de amostras da classe $i$ no nó $t$ e $C$ é o número de classes.

    \item \textbf{Naive Bayes Gaussian}: Baseado no teorema de Bayes, este modelo assume a independência entre as características.
    \[
        P(y|x_1, \ldots, x_n) \propto P(y) \prod_{i=1}^{n} P(x_i|y)
    \]
    onde $P(x_i|y)$ é a probabilidade condicional de $x_i$ dada a classe $y$, assumindo uma distribuição normal.

    \item \textbf{K-Nearest Neighbors (KNN)}: Classifica os dados com base na proximidade dos pontos de dados.
    \[
        \hat{y} = \arg\max_{c} \sum_{i=1}^{k} I(y_i = c)
    \]
    onde $I(\cdot)$ é a função indicadora, $y_i$ é a classe do $i$-ésimo vizinho mais próximo e $c$ é uma classe candidata.

    \item \textbf{Multi-layer Perceptron (MLP)}: Uma rede neural artificial com uma ou mais camadas ocultas.
    A função de ativação ReLU é dada por:

    \[
        \text{ReLU}(x) = \max(0, x)
    \]
    e a função de ativação softmax é dada por:

    \[
        \text{softmax}(z_i) = \frac{e^{z_i}}{\sum_{j} e^{z_j}}
    \]